{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from net.modules import *\n",
    "from net.unet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNetBaseline(\n",
       "  (layers): ModuleList(\n",
       "    (0): DoubleConv(\n",
       "      (net): Sequential(\n",
       "        (0): Conv1D(\n",
       "          (conv): Conv1d(1, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (net): Sequential(\n",
       "            (0): Conv1d(1, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "            (1): BNN1D(\n",
       "              (bn1d): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (2): PReLU(num_parameters=1)\n",
       "          )\n",
       "        )\n",
       "        (1): AttentionLayer(in_channels=16, out_channels=16, key_channels=16)\n",
       "      )\n",
       "    )\n",
       "    (1): Down(\n",
       "      (net): Sequential(\n",
       "        (0): DoubleConv(\n",
       "          (net): Sequential(\n",
       "            (0): Conv1D(\n",
       "              (conv): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              (net): Sequential(\n",
       "                (0): Conv1d(16, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "                (1): BNN1D(\n",
       "                  (bn1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (2): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "            (1): AttentionLayer(in_channels=32, out_channels=32, key_channels=32)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): Down(\n",
       "      (net): Sequential(\n",
       "        (0): DoubleConv(\n",
       "          (net): Sequential(\n",
       "            (0): Conv1D(\n",
       "              (conv): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              (net): Sequential(\n",
       "                (0): Conv1d(32, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "                (1): BNN1D(\n",
       "                  (bn1d): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (2): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "            (1): AttentionLayer(in_channels=64, out_channels=64, key_channels=64)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): Down(\n",
       "      (net): Sequential(\n",
       "        (0): DoubleConv(\n",
       "          (net): Sequential(\n",
       "            (0): Conv1D(\n",
       "              (conv): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              (net): Sequential(\n",
       "                (0): Conv1d(64, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "                (1): BNN1D(\n",
       "                  (bn1d): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (2): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "            (1): AttentionLayer(in_channels=128, out_channels=128, key_channels=128)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): Down(\n",
       "      (net): Sequential(\n",
       "        (0): DoubleConv(\n",
       "          (net): Sequential(\n",
       "            (0): Conv1D(\n",
       "              (conv): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              (net): Sequential(\n",
       "                (0): Conv1d(128, 256, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "                (1): BNN1D(\n",
       "                  (bn1d): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "                )\n",
       "                (2): PReLU(num_parameters=1)\n",
       "              )\n",
       "            )\n",
       "            (1): AttentionLayer(in_channels=256, out_channels=256, key_channels=256)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (5): Up(\n",
       "      (upsample): Deconv1D(\n",
       "        (deconv): ConvTranspose1d(256, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (net): Sequential(\n",
       "          (0): ConvTranspose1d(256, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (1): BNN1D(\n",
       "            (bn1d): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): PReLU(num_parameters=1)\n",
       "        )\n",
       "      )\n",
       "      (conv): DoubleConv(\n",
       "        (net): Sequential(\n",
       "          (0): Conv1D(\n",
       "            (conv): Conv1d(256, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "            (net): Sequential(\n",
       "              (0): Conv1d(256, 128, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              (1): BNN1D(\n",
       "                (bn1d): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (2): PReLU(num_parameters=1)\n",
       "            )\n",
       "          )\n",
       "          (1): AttentionLayer(in_channels=128, out_channels=128, key_channels=128)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): Up(\n",
       "      (upsample): Deconv1D(\n",
       "        (deconv): ConvTranspose1d(128, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (net): Sequential(\n",
       "          (0): ConvTranspose1d(128, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (1): BNN1D(\n",
       "            (bn1d): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): PReLU(num_parameters=1)\n",
       "        )\n",
       "      )\n",
       "      (conv): DoubleConv(\n",
       "        (net): Sequential(\n",
       "          (0): Conv1D(\n",
       "            (conv): Conv1d(128, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "            (net): Sequential(\n",
       "              (0): Conv1d(128, 64, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              (1): BNN1D(\n",
       "                (bn1d): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (2): PReLU(num_parameters=1)\n",
       "            )\n",
       "          )\n",
       "          (1): AttentionLayer(in_channels=64, out_channels=64, key_channels=64)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (7): Up(\n",
       "      (upsample): Deconv1D(\n",
       "        (deconv): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (net): Sequential(\n",
       "          (0): ConvTranspose1d(64, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (1): BNN1D(\n",
       "            (bn1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): PReLU(num_parameters=1)\n",
       "        )\n",
       "      )\n",
       "      (conv): DoubleConv(\n",
       "        (net): Sequential(\n",
       "          (0): Conv1D(\n",
       "            (conv): Conv1d(64, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "            (net): Sequential(\n",
       "              (0): Conv1d(64, 32, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              (1): BNN1D(\n",
       "                (bn1d): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (2): PReLU(num_parameters=1)\n",
       "            )\n",
       "          )\n",
       "          (1): AttentionLayer(in_channels=32, out_channels=32, key_channels=32)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (8): Up(\n",
       "      (upsample): Deconv1D(\n",
       "        (deconv): ConvTranspose1d(32, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "        (net): Sequential(\n",
       "          (0): ConvTranspose1d(32, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "          (1): BNN1D(\n",
       "            (bn1d): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (2): PReLU(num_parameters=1)\n",
       "        )\n",
       "      )\n",
       "      (conv): DoubleConv(\n",
       "        (net): Sequential(\n",
       "          (0): Conv1D(\n",
       "            (conv): Conv1d(32, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "            (net): Sequential(\n",
       "              (0): Conv1d(32, 16, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "              (1): BNN1D(\n",
       "                (bn1d): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              )\n",
       "              (2): PReLU(num_parameters=1)\n",
       "            )\n",
       "          )\n",
       "          (1): AttentionLayer(in_channels=16, out_channels=16, key_channels=16)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNetBaseline()\n",
    "x = torch.randn(4, 1, 100)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNETNiLM()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(4, 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s, p=model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
