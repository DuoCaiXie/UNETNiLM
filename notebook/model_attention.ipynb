{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "sys.path.append('../src/')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from net.modules import *\n",
    "from data.load_data import *\n",
    "from data.data_loader import load_data , spilit_refit_test, Seq2PointDataset\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "    def forward(self, x):\n",
    "        return self.fn(self.norm(x))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim=50, heads = 8):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = dim ** -0.5\n",
    "        self.to_qkv = nn.Linear(dim,dim*3*heads, bias = False)\n",
    "        self.to_out = nn.Linear(dim*heads, dim)\n",
    "    def forward(self, x):\n",
    "        b, n, _, h = *x.shape, self.heads\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = torch.split(qkv, qkv.size(-1)//3, dim=-1)\n",
    "        dots = torch.matmul(q.view(b, n, self.heads, -1), k.view(b, n, -1, self.heads)) *self.scale \n",
    "        attn = dots.softmax(dim=-1)\n",
    "        out = torch.matmul(attn, q.view(b, n, self.heads, -1))     # (bs, n_heads, q_length, dim_per_head)\n",
    "        out = rearrange(out, 'b c h d -> b c (h d)')\n",
    "        out =  self.to_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.1073, -0.1985,  0.1542,  0.1815, -0.1100, -0.0326,  0.2754,\n",
       "           0.0006, -0.0403, -0.0490, -0.0900,  0.0306,  0.2606, -0.1323,\n",
       "           0.0563, -0.0187,  0.1833, -0.1931,  0.1775,  0.0090,  0.2378,\n",
       "          -0.0916,  0.0474, -0.0295,  0.1758, -0.2126,  0.0046, -0.2506,\n",
       "           0.1311, -0.2287, -0.0642,  0.3006,  0.1802, -0.0861,  0.0818,\n",
       "          -0.1271, -0.2213, -0.0790, -0.0996, -0.1093, -0.0216,  0.3117,\n",
       "           0.1757,  0.0943, -0.0021,  0.0685, -0.0405, -0.0251, -0.1404,\n",
       "           0.0457]],\n",
       "\n",
       "        [[-0.0704,  0.0647,  0.0489,  0.0787,  0.0412,  0.0649, -0.0862,\n",
       "           0.0240,  0.1134,  0.0090, -0.2821, -0.3184,  0.0333,  0.0144,\n",
       "           0.0993,  0.0212, -0.0031, -0.0221, -0.1573, -0.0652, -0.1274,\n",
       "          -0.0782,  0.0542,  0.1586,  0.1367, -0.1701, -0.1496,  0.0571,\n",
       "           0.1322,  0.1875, -0.2084, -0.1738, -0.1797, -0.0248, -0.0302,\n",
       "          -0.1544,  0.0716, -0.0808,  0.0748,  0.1718,  0.0466, -0.0147,\n",
       "          -0.0671, -0.0896, -0.2289, -0.1163, -0.2641,  0.1528,  0.0712,\n",
       "          -0.0815]],\n",
       "\n",
       "        [[-0.0374, -0.0361,  0.2848, -0.0346,  0.0059, -0.0724, -0.0981,\n",
       "          -0.1115,  0.0222, -0.0473,  0.0919, -0.1400, -0.1019,  0.1734,\n",
       "           0.0330, -0.1459,  0.0853, -0.0688, -0.1107,  0.0551, -0.0532,\n",
       "           0.1143, -0.0666,  0.0352,  0.0920,  0.0522, -0.1821,  0.0633,\n",
       "           0.0286,  0.0732, -0.1307,  0.1026, -0.1806,  0.0558,  0.0693,\n",
       "           0.1492,  0.0342, -0.1106,  0.0323, -0.1207, -0.0506, -0.0683,\n",
       "          -0.0097,  0.0138, -0.0434,  0.0681, -0.1556, -0.0661,  0.1075,\n",
       "           0.0310]],\n",
       "\n",
       "        [[ 0.0347, -0.0744, -0.0745, -0.1211, -0.0782, -0.1905, -0.1491,\n",
       "          -0.2282, -0.0682,  0.0890,  0.2341, -0.1626, -0.1996,  0.0511,\n",
       "          -0.1017, -0.0661, -0.0664,  0.1214,  0.1343, -0.0510,  0.0702,\n",
       "          -0.0793, -0.2382,  0.1354,  0.0709,  0.1176, -0.1046,  0.0120,\n",
       "          -0.0125, -0.0073,  0.2613,  0.1101, -0.0764, -0.1332, -0.1227,\n",
       "           0.1568, -0.2038, -0.0046, -0.1054,  0.1646, -0.3164,  0.0138,\n",
       "          -0.0292, -0.0918, -0.1286,  0.1802,  0.0161, -0.0067,  0.0369,\n",
       "           0.2775]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = Attention() \n",
    "x = torch.randn(4, 1, 50)\n",
    "model(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NILMiT(nn.Module):\n",
    "    def __init__(self, seq_len=50, patch_size=5, num_classes=12, \n",
    "                 dim=50, depth=4, heads=8, \n",
    "                 mlp_dim=128, channels = 1, n_quantiles=3):\n",
    "        super().__init__()\n",
    "        assert seq_len % patch_size == 0, 'image dimensions must be divisible by the patch size'\n",
    "        num_patches = (seq_len // patch_size) \n",
    "        patch_dim = channels * patch_size \n",
    "        self.patch_size = patch_size\n",
    "        self.n_quantiles = n_quantiles\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, num_patches + 1, dim))\n",
    "        self.patch_to_embedding = nn.Linear(patch_dim, dim)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.transformer = Transformer(dim, depth, heads, mlp_dim)\n",
    "        self.to_cls_token = nn.Identity()\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, num_classes)\n",
    "        )\n",
    "        self.mlp_classifier = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, num_classes*2))\n",
    "        \n",
    "        self.mlp_regress = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_dim, num_classes*n_quantiles))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        p = self.patch_size\n",
    "        B = x.size(0)\n",
    "        x  = rearrange(x.unsqueeze(-1), 'b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = p, p2 = 1)\n",
    "        x  = self.patch_to_embedding(x)\n",
    "        cls_tokens = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "        \n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x += self.pos_embedding\n",
    "        x = self.transformer(x)\n",
    "        x = self.to_cls_token(x[:, 0])\n",
    "        \n",
    "        states_logits   = self.mlp_classifier(x).reshape(B, 2, -1)\n",
    "        power_logits    = self.mlp_regress(x)\n",
    "        if self.n_quantiles>1:\n",
    "            power_logits = power_logits.reshape(B, self.n_quantiles, -1)\n",
    "        return states_logits, power_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 0.1269, -0.1844, -0.2307,  0.0641, -0.6315,  0.2955,  0.2687,\n",
       "            0.5740,  0.1166,  0.1411, -0.0677, -0.0164],\n",
       "          [ 0.0956, -0.0302,  0.0058, -0.2058,  0.0164,  0.1673, -0.4383,\n",
       "           -0.0162, -0.3665,  0.0420, -0.1358, -0.0666]],\n",
       " \n",
       "         [[ 0.1269, -0.1844, -0.2307,  0.0641, -0.6315,  0.2955,  0.2687,\n",
       "            0.5740,  0.1166,  0.1411, -0.0677, -0.0164],\n",
       "          [ 0.0956, -0.0302,  0.0058, -0.2058,  0.0164,  0.1673, -0.4383,\n",
       "           -0.0162, -0.3665,  0.0420, -0.1358, -0.0666]],\n",
       " \n",
       "         [[ 0.1269, -0.1844, -0.2307,  0.0641, -0.6315,  0.2955,  0.2687,\n",
       "            0.5740,  0.1166,  0.1411, -0.0677, -0.0164],\n",
       "          [ 0.0956, -0.0302,  0.0058, -0.2058,  0.0164,  0.1673, -0.4383,\n",
       "           -0.0162, -0.3665,  0.0420, -0.1358, -0.0666]],\n",
       " \n",
       "         [[ 0.1269, -0.1844, -0.2307,  0.0641, -0.6315,  0.2955,  0.2687,\n",
       "            0.5740,  0.1166,  0.1411, -0.0677, -0.0164],\n",
       "          [ 0.0956, -0.0302,  0.0058, -0.2058,  0.0164,  0.1673, -0.4383,\n",
       "           -0.0162, -0.3665,  0.0420, -0.1358, -0.0666]]],\n",
       "        grad_fn=<ViewBackward>),\n",
       " tensor([[[ 0.1864,  0.2962,  0.1385,  0.2778, -0.0421, -0.0113,  0.3692,\n",
       "           -0.0635, -0.1443,  0.4049,  0.4171, -0.2431],\n",
       "          [-0.2098,  0.3366,  0.1827, -0.4511,  0.2512,  0.3219, -0.0542,\n",
       "            0.1105,  0.2750, -0.0249, -0.1691,  0.2234],\n",
       "          [ 0.4925,  0.3222,  0.2962, -0.3112, -0.0049,  0.1780, -0.0638,\n",
       "           -0.2519, -0.0304, -0.0721, -0.2460, -0.4568]],\n",
       " \n",
       "         [[ 0.1864,  0.2962,  0.1385,  0.2778, -0.0421, -0.0113,  0.3692,\n",
       "           -0.0635, -0.1443,  0.4049,  0.4171, -0.2431],\n",
       "          [-0.2098,  0.3366,  0.1827, -0.4511,  0.2512,  0.3219, -0.0542,\n",
       "            0.1105,  0.2750, -0.0249, -0.1691,  0.2234],\n",
       "          [ 0.4925,  0.3222,  0.2962, -0.3112, -0.0049,  0.1780, -0.0638,\n",
       "           -0.2519, -0.0304, -0.0721, -0.2460, -0.4568]],\n",
       " \n",
       "         [[ 0.1864,  0.2962,  0.1385,  0.2778, -0.0421, -0.0113,  0.3692,\n",
       "           -0.0635, -0.1443,  0.4049,  0.4171, -0.2431],\n",
       "          [-0.2098,  0.3366,  0.1827, -0.4511,  0.2512,  0.3219, -0.0542,\n",
       "            0.1105,  0.2750, -0.0249, -0.1691,  0.2234],\n",
       "          [ 0.4925,  0.3222,  0.2962, -0.3112, -0.0049,  0.1780, -0.0638,\n",
       "           -0.2519, -0.0304, -0.0721, -0.2460, -0.4568]],\n",
       " \n",
       "         [[ 0.1864,  0.2962,  0.1385,  0.2778, -0.0421, -0.0113,  0.3692,\n",
       "           -0.0635, -0.1443,  0.4049,  0.4171, -0.2431],\n",
       "          [-0.2098,  0.3366,  0.1827, -0.4511,  0.2512,  0.3219, -0.0542,\n",
       "            0.1105,  0.2750, -0.0249, -0.1691,  0.2234],\n",
       "          [ 0.4925,  0.3222,  0.2962, -0.3112, -0.0049,  0.1780, -0.0638,\n",
       "           -0.2519, -0.0304, -0.0721, -0.2460, -0.4568]]],\n",
       "        grad_fn=<ViewBackward>))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NILMiT()\n",
    "x = torch.randn(4, 1, 50)\n",
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-head attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
